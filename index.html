<!DOCTYPE html>

<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Renrui Zhang's home page"> 
  
  <link href="./wfdoc.css" rel="stylesheet" type="text/css"> 
  <title>Renrui Zhang's Homepage</title> 
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Renrui Zhang</h1></div>
        <h3>Research Scientist</h3>
        <p><a href="https://seed.bytedance.com/en/">Seed, ByteDance</a>, San Jose, USA
        <br>
        <h3>Ph.D.</h3>
        <p><a href="http://mmlab.ie.cuhk.edu.hk/">MMLab</a>, CUHK, Hong Kong, China
        <br>
        <br> Email:  
        <a href="mailto:1700012927zrr@gmail.com">     1700012927zrr@gmail.com</a>
        <br>
        <br> [<a href="https://scholar.google.com/citations?user=YlL3xN4AAAAJ&hl=en">Google Scholar</a>]&nbsp;&nbsp[<a href="https://github.com/ZrrSkywalker">GitHub</a>]
        </p>
      </td>
      <td>
        <div>
          <img width="270" src="./bio.jpg" border="0">
        </div>
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">


<h2>Education</h2>
  <ul>
    <li>
      [2017-2021] ğŸ‰ I received my B.E. from <a href="https://english.pku.edu.cn/">Peking University</a>, awarded Outstanding Graduate (Top 5%).
    </li>
    <li>
      [2020-2021] I worked as a visiting student in University of Pennsylvania, supervised by <a href="https://scholar.google.com/citations?user=Sm14jYIAAAAJ&hl=zh-CN&oi=ao">Prof. Jianbo Shi</a>.
    </li>
    <li>
      [2022-2025] ğŸ“ I obtained my Ph.D. in MMLab, CUHK, supervised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a> and <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en">Prof. Xiaogang Wang</a>.
    </li>
    <li>
      [2021-2024] I worked as a research intern at Shanghai AI Lab, supervised by <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=zh-CN">Dr. Peng Gao</a>.
    </li>
    <li>
      [2024-2025] I worked as a research intern at <a href="https://llava-vl.github.io/blog/">LLaVA team</a>, ByteDance, Seattle, supervised by <a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&hl=zh-CN">Dr. Chunyuan Li</a>.
    </li>
    <li>
      [2025-Now] ğŸ’ª I joined <a href="https://seed.bytedance.com/en/">Seed, ByteDance</a>, San Jose, as a research scientist, developing <a href="https://seed.bytedance.com/en/seed1_6">Doubao-Seed Models</a>.
    </li>
</ul>
    
<h2>Biography</h2>
  <p>
    <br><br> ğŸ“Œ My research interests include <i>Large Multimodal Models</i>, <i>Vision-language Learning</i>, <i>Emboided AI</i>, and <i>3D Vision</i>.
    <br><br> âœ‰ï¸ I'm looking for undergraduate and graduate students for academic cooperation. Discussions are welcome!
  </p>

  <h2>News</h2>
<ul>
  <li>
    [2025-09] ğŸ‰ I am selected as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6">World's Top 2% Scientists</a> by Stanford University.
  </li>
  <li>
    [2025-09] Nine papers accepted by NeurIPS 2025
  </li>
  <li>
    [2025-06] ğŸ† We release <a href="https://seed.bytedance.com/en/seed1_6">"Seed1.6"</a>, <b>with advanced reasoning and agentic capabilities</b>.
  </li>
  <li>
    [2025-05] ğŸ† We release <a href="https://seed.bytedance.com/zh/tech/seed1_5_vl">"Seed1.5-VL"</a>, <b>a series of state-of-the-art multimodal large models</b>.
  </li>
  <li>
    [2025-05] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2505.00703">"T2I-R1"</a>, <b>introducing R1 into image generation domains for CoT reasoning</b>.
  </li>
  <li>
    [2025-05] One paper accepted by ICML 2025
  </li>
  <li>
    [2025-03] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2503.10631">"HybridVLA"</a>, the first work <b>unifying Autoregression and Diffusion in VLA models</b>.
  </li>
  <li>
    [2025-02] Three papers accepted by CVPR 2025, one Highlight ğŸ‰
  </li>
  <li>
    [2025-01] Five papers accepted by ICLR 2025, two Spotlight ğŸ‰
  </li>
  <li>
    [2025-01] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2501.13926">"Image Generation with CoT"</a>, the first work investigating <b>CoT strategies (e.g., Test-time Scling, RL, and Reflection)</b> in autoregressive <b>text-to-image generation</b>.
  </li>
  <li>
    [2025-01] ğŸ‰ <a href="https://arxiv.org/pdf/2405.21075">"Video-MME"</a>, is thrilled to be selected as <b>One of the 14 Groundbreaking Stuides in 2024</b>.
  </li>
  <li>
    [2024-08] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2408.03326?">"LLaVA-OneVision"</a>, <b>the latest LLaVA model</b> for image, video, and image-text interleaved scenarios with superior performance.
  </li>
  <li>
    [2024-07] Four papers accepted by ECCV 2024
  </li>
  <li>
    [2024-07] ğŸ”¥ We release <a href="https://openreview.net/pdf?id=oSQiao9GqB">"LLaVA-NeXT-Interleave"</a> for multi-image instruction tuning and <a href="https://arxiv.org/pdf/2407.08739">"MAVIS"</a> for multimodal mathematical reasoning.
  </li>
  <li>
    [2024-05] Three papers accepted by ICML 2024
  </li>
  <li> 
[2024-03] Seven papers accepted by CVPR 2024, two Highlight ğŸ‰
</li>
<li>
  [2024-03] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2403.14624">"MathVerse"</a>, a novel mathematical benchmark with the first CoT evaluation strategy.
</li>
<li>
[2024-01] Four papers accepted by ICLR 2024
</li>
</ul>



<h2>Selected Projects</h2>
* Equal Contribution &nbsp;&nbsp # Project Lead

<br><br><tr><td><b><font color="##36BC2">&spades; o1/R1-like Chain-of-Thought (CoT) Reasoning </font></b></td></tr>

    
<ul>
    <li>
      <div style="margin-top: 30px;"><b>ğŸ”¥ Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Ziyu Guo*, <u><b>Renrui Zhang#*</b></u>, Chengzhuo Tong*, Zhizheng Zhao*, Haoquan Zhang, Manyuan Zhang, Peng Gao, Hongsheng Li, Pheng-Ann Heng</a></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The first work investigating CoT strategies (e.g., Test-time Scling, RL, and Reflection) in autoregressive T2I</i></a></div>
        <div style="margin-top: 5px;"><i><b>CVPR 2025</b></i><br></div>
        <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2501.13926">Paper</a>]
        [<a href="https://github.com/ZiyuGuo99/Image-Generation-CoT">Code ğŸŒŸ500+</a>]</div>
      </td>
    </li>
  </ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>ğŸ”¥ MME-CoT: Benchmarking CoT in LMMs for Reasoning Quality, Robustness, and Efficiency</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Dongzhi Jiang*, <u><b>Renrui Zhang#*</b></u>, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The first-ever comprehensive benchmark to evaluate CoT reasoning capabilities in LMMs</i></a></div>
      <div style="margin-top: 5px;"><i><b>arXiv 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2502.09621">Paper</a>]
      [<a href="https://mmecot.github.io/">Project</a>]
      [<a href="https://github.com/CaraJ7/MME-CoT">Code</a>]</div>
    </td>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang#*</b></u>, Xinyu Wei*, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao, Chunyuan Li, Hongsheng Li</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The first specialized LMM for multimodal mathematical problem-solving (CLIP-Math + CoT SFT + DPO)</i></a></div>
      <div style="margin-top: 5px;"><i><b>ICLR 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2407.08739">Paper</a>]
      [<a href="https://github.com/ZrrSkywalker/MAVIS">Code</a>]</div>
    </td>
  </li>
</ul>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang#*</b></u>, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>A novel benchmark assessing visual comprehension and CoT capabilities of LMMs for mathematics</i></a></div>
      <div style="margin-top: 5px;"><i><b>ECCV 2024</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2403.14624">Paper ğŸ“100+</a>]
      [<a href="https://mathverse-cuhk.github.io/">Project</a>]
      [<a href="https://github.com/ZrrSkywalker/MathVerse">Code</a>]</div>
    </td>
  </li>
</ul>


<br><br><tr><td><b><font color="##36BC2">&spades; Large Language & Multimodal Models (LLMs & LMMs) </font></b></td></tr>

<ul>
    <li>
      <div style="margin-top: 30px;"><b>ğŸ”¥ LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-initialized Attention</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang*</b></u>, Jiaming Han*, Dongyang Liu*, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, Peng Gao</a></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The pioneering work performing visual instruction tuning for LLMs, prior to LLaVA and MiniGPT-4</i></a></div>
        <div style="margin-top: 5px;"><i><b>ICLR 2024</b></i><br></div>
        <div style="margin-top: 5px;">[<a href="https://openreview.net/pdf?id=d4UiXAHN2W">Paper ğŸ“800+</a>]
        [<a href="https://github.com/OpenGVLab/LLaMA-Adapter">Code ğŸŒŸ5.8K+</a>]</div>
      </td>
    </li>
  </ul>

  <ul>
    <li>
      <div style="margin-top: 30px;"><b>ğŸ”¥ LLaVA-OneVision: Easy Visual Task Transfer</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Bo Li, Yuanhan Zhang, Dong Guo, <u><b>Renrui Zhang</b></u>, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li</a></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The latest, open-sourced LLaVA model superior at image, video, and image-text interleaved scenarios</i></a></div>
        <div style="margin-top: 5px;"><i><b>TMLR 2025</b></i><br></div>
        <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2408.03326?">Paper ğŸ“400+</a>]
          [<a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">Project</a>]
        [<a href="https://github.com/LLaVA-VL/LLaVA-NeXT">Code ğŸŒŸ3.6K+</a>]</div>
      </td>
    </li>
  </ul>


  <ul>
    <li>
      <div style="margin-top: 30px;"><b>ğŸ”¥ LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Feng Li*, <u><b>Renrui Zhang*</b></u>, Hao Zhang*, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, Chunyuan Li</a></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The upgraded LLaVA model handling image-text interleaved, multi-image, video, and 3D tasks</i></a></div>
        <div style="margin-top: 5px;"><i><b>ICLR 2024 Spotlight ğŸ‰</b></i><br></div>
        <div style="margin-top: 5px;">[<a href="https://openreview.net/pdf?id=oSQiao9GqB">Paper ğŸ“100+</a>]
          [<a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">Project</a>]
        [<a href="https://github.com/LLaVA-VL/LLaVA-NeXT">Code ğŸŒŸ3.6K+</a>]</div>
      </td>
    </li>
  </ul>


  <ul>
    <li>
      <div style="margin-top: 30px;"><b>Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Ziyu Guo*, <u><b>Renrui Zhang#*</b></u>, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng</a></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>Point-LLM, the first LLM in 3D world, prior to 3D-LLM and PointLLM, and Point-Bind, a multi-modal 3D large model</i></a></div>
        <div style="margin-top: 5px;"><i><b>arXiv 2024</b></i><br></div>
        <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2309.00615">Paper ğŸ“100+</a>]
        [<a href="https://github.com/ZiyuGuo99/Point-Bind_Point-LLM">Code ğŸŒŸ400+</a>]</div>
      </td>
    </li>
  </ul>


  <ul>
    <li>
      <div style="margin-top: 30px;"><b>Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Chaoyou Fu#, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, <u><b>Renrui Zhang</b></u>, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun</a></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>A comprehensive and thorough benchmark for evaluating LMMs in video understanding</i></a></div>
        <div style="margin-top: 5px;"><i><b>CVPR 2025</b></i><br></div>
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i><b>ğŸ‰ Selected as One of the 14 Groundbreaking Stuides in 2024</b></i></a><br></div>
        <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2405.21075">Paper ğŸ“200+</a>]
          [<a href="https://video-mme.github.io/home_page.html">Project</a>]
        [<a href="https://github.com/BradyFU/Video-MME">Code ğŸŒŸ400+</a>]</div>
      </td>
    </li>
  </ul>


  <ul>
    <li>
      <div style="margin-top: 30px;"><b>MMSearch: Unveiling the Potential of Large Models as Multi-modal Search Engines</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Dongzhi Jiang*, <u><b>Renrui Zhang#*</b></u>, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li</a></div>
      <div style="margin-top: 5px;"><i><b>ICLR 2025</b></i><br></div>
    </li>
  </ul>

  <ul>
    <li>
      <div style="margin-top: 30px;"><b>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Dongyang Liu*, <u><b>Renrui Zhang*</b></u>, Longtian Qiu*, Siyuan Huang*, Weifeng Lin*, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao, Peng Gao</a></div>
      <div style="margin-top: 5px;"><i><b>ICML 2024</b></i><br></div>
    </li>
  </ul>

  <ul>
    <li>
      <div style="margin-top: 30px;"><b>ImageBind-LLM: Multi-modality Instruction Tuning</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Jiaming Han*, <u><b>Renrui Zhang*</b></u>, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao</a></div>
      <div style="margin-top: 5px;"><i><b>arXiv 2023</b></i><br></div>
    </li>
  </ul>





<br><br><tr><td><b><font color="##36BC2">&spades; Large Vision Models </font></b></td></tr>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>ğŸ”¥ Personalize Segment Anything Model with One Shot</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang</b></u>, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>Customize SAM model to segment personal concepts in a zero-shot or parameter-efficient manner</i></a></div>
      <div style="margin-top: 5px;"><i><b>ICLR 2024</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2305.03048">Paper ğŸ“200+</a>]
      [<a href="https://github.com/ZrrSkywalker/Personalize-SAM">Code ğŸŒŸ1.6K+</a>]</div>
    </td>
  </li>
</ul>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>ğŸ”¥ SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Ziyu Guo*, <u><b>Renrui Zhang#*</b></u>, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The "Segment Anything Moment" in 3D world for 3D objects, indoor & outdoor scenes, and raw LiDARs</i></a></div>
      <div style="margin-top: 5px;"><i><b>arXiv 2024</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2408.16768">Paper</a>]
        [<a href="https://sam2point.github.io/">Project</a>]
        [<a href="https://huggingface.co/spaces/ZiyuG/SAM2Point">Live Demo ğŸ¨</a>]
      [<a href="https://github.com/ZiyuGuo99/SAM2Point">Code ğŸŒŸ300+</a>]</div>
    </td>
  </li>
</ul>


<br><br><tr><td><b><font color="##36BC2">&spades; Emboided AI & Robotics </font></b></td></tr>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>ğŸ”¥ HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Jiaming Liu*, Hao Chen*, Pengju An, Zhuoyang Liu, <u><b>Renrui Zhang#</b></u>, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC Zhou, Pheng-Ann Heng, Shanghang Zhang</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The first unified autoregression and diffusion framework in VLA models</i></a></div>
      <div style="margin-top: 5px;"><i><b>arXiv 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2503.10631">Paper</a>]
        [<a href="https://hybrid-vla.github.io/">Project</a>]
      [<a href="https://github.com/PKU-HMI-Lab/Hybrid-VLA">Code ğŸŒŸ100+</a>]</div>
    </td>
  </li>
</ul>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>Lift3D Foundation Policy: Lifting 2D Large-scale Pretrained Models for Robust 3D Robotic Manipulation</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Yueru Jia*, Jiaming Liu*#, Sixiang Chen*, Chenyang Gu, Zhilue Wang, Longzan Luo, Lily Lee, Pengwei Wang, Zhongyuan Wang, <u><b>Renrui Zhang#</b></u>, Shanghang Zhang</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>Adapting frozen 2D pre-trained models effectively for robust 3D robotic manipulation</i></a></div>
      <div style="margin-top: 5px;"><i><b>CVPR 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2411.18623">Paper</a>]
        [<a href="https://lift3d-web.github.io/">Project</a>]
      [<a href="https://github.com/PKU-HMI-Lab/LIFT3D">Code ğŸŒŸ100+</a>]</div>
    </td>
  </li>
</ul>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Jiaming Liu*#, Mengzhen Liu*, Zhenyu Wang, Lily Lee, Kaichen Zhou, Pengju An, Senqiao Yang, <u><b>Renrui Zhang#</b></u>, Yandong Guo, Shanghang Zhang</a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>Applying Mamba architecture with a simple policy head for efficient robotic manipulation</i></a></div>
      <div style="margin-top: 5px;"><i><b>NeurIPS 2024</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/46a126492ea6fb87410e55a58df2e189-Paper-Conference.pdf">Paper</a>]
        [<a href="https://sites.google.com/view/robomamba-web">Project</a>]
      [<a href="https://github.com/lmzpai/roboMamba">Code ğŸŒŸ100+</a>]</div>
    </td>
  </li>
</ul>


<br><br><tr><td><b><font color="##36BC2">&spades; Vision-language Learning </font></b></td></tr>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>PointCLIP: Point Cloud Understanding by CLIP</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang*</b></u>, Ziyu Guo*, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li</a></div>
    <div style="margin-top: 5px;"><i><b>CVPR 2022</b></i><br></div>
  </li>
</ul>


<ul>
  <li>
    <div style="margin-top: 30px;"><b>Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang*</b></u>, Wei Zhang*, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li</a></div>
    <div style="margin-top: 5px;"><i><b>ECCV 2022</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang*</b></u>, Xiangfei Hu*, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao</a></div>
    <div style="margin-top: 5px;"><i><b>CVPR 2023</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;">Xiangyang Zhu*, <u><b>Renrui Zhang#*</b></u>, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, Peng Gao</a></div>
    <div style="margin-top: 5px;"><i><b>ICCV 2023</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;">Xiangyang Zhu*, <u><b>Renrui Zhang#*</b></u>, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, Peng Gao</a></div>
    <div style="margin-top: 5px;"><i><b>ICCV 2023</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>CLIP-Adapter: Better Vision-language Models with Feature Adapters</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;">Peng Gao*, Shijie Geng*, <u><b>Renrui Zhang*</b></u>, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao</a></div>
    <div style="margin-top: 5px;"><i><b>IJCV 2024</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;">Yulu Gan, Xianzheng Ma, Yihang Lou, Yan Bai, <u><b>Renrui Zhang</b></u>, Nian Shi, Lin Luo</a></div>
    <div style="margin-top: 5px;"><i><b>AAAI 2023 Best Student Paper ğŸ‰</b></i><br></div>
  </li>
</ul>

<br><br><tr><td><b><font color="##36BC2">&spades; 3D Vision & Autonomous Driving </font></b></td></tr>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang</b></u>, Han Qiu, Tai Wang, Xuanzhuo Xu, Ziyu Guo, Yu Qiao, Peng Gao, Hongsheng Li</a></div>
    <div style="margin-top: 5px;"><i><b>ICCV 2023</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang</b></u>, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, Hongsheng Li</a>
    <div style="margin-top: 5px;"><i><b>NeurIPS 2022</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang</b></u>, Liuhui Wang, Yu Qiao, Peng Gao, Hongsheng Li</a>
    <div style="margin-top: 5px;"><i><b>CVPR 2023</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Renrui Zhang</b></u>, Liuhui Wang, Yali Wang, Peng Gao, Hongsheng Li, Jianbo Shi</a></div>
    <div style="margin-top: 5px;"><i><b>CVPR 2023</b></i><br></div>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation</b></div>
    <div style="margin-top: 5px;"><a style="color: #777;">Xiangyang Zhu*, <u><b>Renrui Zhang#*</b></u>, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao</a>
    <div style="margin-top: 5px;"><i><b>CVPR 2024 Hightlight ğŸ‰</b></i><br></div>
  </li>
</ul>



<h2>Selected Awards</h2>
<ul>
<li> 
  [2021-06] Outstanding Graduate, Peking University (Top <b>5%</b>)
</li>
<li> 
[2020-09] Academic Excellent Scholarship (Ranked <b>1<sup>st</sup></b>/73)
</li>
<li> 
[2020-09] Merit Student PaceSetter, Peking University (Ranked <b>1<sup>st</sup></b>/73)
</li>
<li> 
[2019-09] Academic Excellent Scholarship (Ranked <b>4<sup>th</sup></b>/73)
</li>
<li> 
[2019-09] Merit Student, Peking University (Ranked <b>4<sup>th</sup></b>/73)
</li>
<li> 
[2016-07] China Youth Technology Innovation Award (The <b>Only 1</b> in Province)
</li>
<li> 
[2016-10] 1<sup>st</sup> Prize in Provincial Chinese Physics Olympiad (Ranked <b>18<sup>th</sup></b> in Province)
</li>
<li> 
[2015-10] 2<sup>nd</sup> Prize in The Chinese 15<sup>th</sup> Awarding Program for Future Scientist (Ranked <b>1<sup>st</sup></b> in Province)
</li>
<li> 
[2013-03] 1<sup>st</sup> Prize in Provincial China Adolescent Robotics Competition (Ranked <b>1<sup>st</sup></b> in Province)
</li>
</ul>

<h2>Hobbies</h2>
Soccer âš½ï¸, Moive ğŸ¬, Singing ğŸ¤, Piano ğŸ¹, Violin ğŸ», Snorkeling ğŸ¤¿, <a href="https://www.hottoys.com.hk/">HotToys</a> ğŸ¦¸â€â™‚ï¸, <a href="https://fco.qq.com/main.shtml">FC Online</a> ğŸ®, <a href="https://www.pubg.com/zh-cn">PUBG</a> ğŸ“
<ul>
</ul>
</td>
</tr>
</table>
<a href="https://clustrmaps.com/site/1bvnp"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=PMNb1-V7zscdFbh7YtW8384Mvl0JzHFryZH1_dEVw5M&cl=ffffff" /></a>
</body>

</div>
</div>


</body></html>
