<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Renrui Zhang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-content">
<div id="toptitle">
<table class="imgtable"><tr><td>
<img src="bioo.jpg" alt="alt text" width="213px" height="160px" />&nbsp;</td>
<td align="left"><p><h1>Renrui Zhang</h1><br />Ph.D. candidate<br /> <br /> 
<a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory (MMLab)</a>, The Chinese University of Hong Kong<br />
Office: SHB 304, CUHK, Hong Kong S.A.R., China <br /> <br />
Email: 1700012927@pku.edu.cn, zhangrenrui@pjlab.org.cn <br /><br />
[<a href="https://scholar.google.com/citations?user=YlL3xN4AAAAJ&hl=zh-CN&authuser=1">Google Scholar</a>], [<a href="https://github.com/ZrrSkywalker">GitHub</a>] </p>
</td></tr></table>
<!-- <h2>Self-intro</h2> -->
  </ul>

  <ul>
ğŸ“Œ My research interests are large language/vision models, multi-modal learning, and 3D vision
</ul>
<ul>
âœ‰ï¸ <b>I anticipate graduating in 2025 for both academic and industrial research positions.</b> If you are interested, please feel free to contact me.
</ul>
<ul>
  âœ‰ï¸ I'm also looking for self-motivated undergraduate and graduate students for academic cooperation.
</ul>
</ul>
<h2>Education</h2>
<ul>
<li><p>[2017-2021] ğŸ‰ I received my B.E. degree from <a href="https://english.pku.edu.cn/">Peking University</a>, awarded Outstanding Graduate (Top 5%).</p>
</li>
<li><p>[2020-2021] I worked as a visiting student in University of Pennsylvania, supervised by <a href="https://scholar.google.com/citations?user=Sm14jYIAAAAJ&hl=zh-CN&oi=ao">Prof. Jianbo Shi</a>.</p>
</li>
<li><p>[2021-Now] ğŸ’ª I'm pursuing my Ph.D. in MMLab, CUHK, supervised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a> and <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=zh-CN">Prof. Xiaogang Wang</a>.</p>
</li>
<li><p>[2021-2024] I worked as a research intern at Shanghai AI Lab, supervised by <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=zh-CN">Dr. Peng Gao</a>.</p>
</li>
<li><p>[2024-Now] I'm working as a research intern at ByteDance, Seattle, supervised by <a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&hl=en">Dr. Chunyuan Li</a>.</p>
</li>
</ul>
<h2>News</h2>
<ul>
<li><p>[2025-02] Three papers accepted by CVPR 2025</p>
</li>
<li><p>[2025-01] Five papers accepted by ICLR 2025, two Spotlight ğŸ‰</p>
</li>
<li><p>[2024-07] Four papers accepted by ECCV 2024</p>
</li>
<li><p>[2024-05] Three papers accepted by ICML 2024</p>
</li>
<li><p>[2024-03] Seven papers accepted by CVPR 2024, two Highlight ğŸ‰</p>
</li>
<li><p>[2024-01] Four papers accepted by ICLR 2024</p>
</li>
<li><p>[2023-09] One paper accepted by NeurIPS 2023</p>
</li>
<li><p>[2023-08] Two papers accepted by IJCV 2023</p>
</li>
<li><p>[2023-07] Five papers accepted by ICCV 2023</p>
</li>
<li><p>[2023-02] Six papers accepted by CVPR 2023</p>
</li>
<li><p>[2022-11] Two papers accepted by AAAI 2023, one Best Student Paper ğŸ‰</p>
</li>
</li>
</ul>
<h2>Selected Preprints and Projects</h2>
<p>* Equal contribution, # Project Lead</p>
<ul>
<li><p><a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision: Easy Visual Task Transfer</a> <br />
B Li, Y Zhang, D Guo, <b>R Zhang</b>, F Li, H Zhang, K Zhang, Y Li, Z Liu, C Li<br />
ğŸ”¥ The new generation of LLaVA models for multi-modal learning</p>
<a href="https://github.com/LLaVA-VL/LLaVA-NeXT">ğŸ”¥ Code [1.4k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2408.16768">SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</a> <br />
Z Guo*, <b>R Zhang*#</b>, X Zhu, C Tong, P Gao, C Li, PA Heng <br />
ğŸš€ The "Segment Anything" moment in 3D point clouds</p>
<a href="https://github.com/ZiyuGuo99/SAM2Point">ğŸ”¥ Code [290+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2309.00615.pdf">Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</a> <br />
Z Guo*, <b>R Zhang*#</b>, X Zhu, Y Tang, X Ma, J Han, K Chen, P Gao, X Li#, H Li, P Heng <br />
ğŸ§  A 3D multi-modal model for general 3D learning, Point-Bind, and the first 3D large language model, Point-LLM</p>
<a href="https://github.com/ZrrSkywalker/Point-Bind_Point-LLM">ğŸ”¥ Code [300+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
  <ul>
<li><p><a href="https://arxiv.org/pdf/2312.12436.pdf">A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise</a> <br />
C Fu*, <b>R Zhang*</b>, H Lin*, Z Wang*, T Gao, Y Luo, Y Huang, Z Zhang, L Qiu, ...<br />
ğŸš€ The first technical report for Gemini vs GPT-4V. A total of 128 pages. Completed within one week of the Gemini API opening</p>
<a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">ğŸ”¥ Code [6.4k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
  <li><p><a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">X-Accessory Code Base</a> <br />
  ğŸ§  An open-source toolkit for the deployment of Large Language Models (LLMs) and multi-modal LLMs</p>
  <a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">ğŸ”¥ Code [2.0k+ Stars ğŸŒŸ]</a><br /></p>
  </li>
  </ul>
<h2>Selected Publications</h2>
<p>* Equal contribution, # Project Lead</p>

  <ul>
<li><p><a href="https://arxiv.org/pdf/2501.13926?">Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</a> <br />
Z Guo*, <b>R Zhang*#</b>, C Tong, Z Zhao, P Gao, H Li, PA Heng <br />
ğŸ§  The first work investigating CoT strategies (e.g., Test-time Scling, RL, and Reflection) in autoregressive T2I </p>
<b>CVPR 2025</b>, <a href="https://github.com/ZiyuGuo99/Image-Generation-CoT">ğŸ”¥ Code [500+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
  
<ul>
<li><p><a href="https://arxiv.org/pdf/2407.08739.pdf">MAVIS: Mathematical Visual Instruction Tuning</a> <br />
<b>R Zhang*</b>, X Wei*, D Jiang, Y Zhang, Z Guo, C Tong, J Liu, A Zhou, B Wei, S Zhang, P Gao, H Li <br />
ğŸ“ The first public large-scale multi-modal mathematical dataset for tuning large models</p>
<b>ICLR 2025</b>, <a href="https://github.com/ZrrSkywalker/MAVIS">ğŸ”¥ Code [50+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
  <ul>
<li><p><a href="https://arxiv.org/pdf/2407.07895.pdf">LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</a> <br />
F Li*, <b>R Zhang*</b>, H Zhang*, Y Zhang, B Li, W Li, Z Ma, C Li <br />
ğŸ’ª The official latest and powerful LLaVA version excelling in diverse multi-modal scenarios</p>
<b>ICLR 2025 Spotlight ğŸ”¥</b>, <a href="https://github.com/LLaVA-VL/LLaVA-NeXT">ğŸ”¥ Code [1.4k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
  <ul>
<li><p><a href="https://mmsearch.github.io/">MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines</a> <br />
D Jiang*, <b>R Zhang*#</b>, Z Guo, Y Wu, J Lei, P Qiu, P Lu, Z Chen, G Song, Y Liu, P Gao, C Li, H Li<br />
ğŸ”¥ The first multi-modal search engine pipeline and benchmark, surpassing Perplexity Pro</p>
<b>ICLR 2025</b>, <a href="https://github.com/CaraJ7/MMSearch">ğŸ”¥ Code [350k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
  <ul>
<li><p><a href="https://arxiv.org/pdf/2403.14624">MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</a> <br />
<b>R Zhang*</b>, D Jiang*, Y Zhang*, H Lin, Z Guo, P Qiu, A Zhou, P Lu, KW Chang, P Gao, H Li <br />
ğŸ“ The first benchmark to evaluate the real capabilities of MLLMs for visual mathematical reasoning</p>
<b>ECCV 2024</b>, <a href="https://github.com/OpenGVLab/LLaMA-Adapter">ğŸ”¥ Code [100k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
  <ul>
<li><p><a href="https://arxiv.org/pdf/2303.16199.pdf">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a> <br />
<b>R Zhang*</b>, J Han*, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao#, Y Qiao <br />
ğŸš€ Fine-tune LLaMA for instruction-following capacity and multi-modal reasoning by PEFT methods</p>
<b>ICLR 2024</b>, <a href="https://github.com/OpenGVLab/LLaMA-Adapter">ğŸ”¥ Code [5.2k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.03048.pdf">Personalize Segment Anything Model with One Shot</a> <br />
<b>R Zhang</b>, Z Jiang*, Z Guo*, S Yan, J Pan, H Dong, P Gao, H Li# <br />
ğŸš€ Efficiently personalize SAM for segmenting specific objects and improving DreamBooth</p>
<b>ICLR 2024</b>, <a href="https://github.com/ZrrSkywalker/Personalize-SAM">ğŸ”¥ Code [1.3k+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
  <li><p><a href="https://arxiv.org/pdf/2203.13310.pdf">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</a> <br />
  <b>R Zhang</b>, H Qiu, T Wang, Z Guo, Z Cui, Y Qiao, P Gao, H Li# <br />
  ğŸš€ The first DETR-based network for camera-based 3D object detection</p>
  <b>ICCV 2023</b>, <a href="https://github.com/ZrrSkywalker/MonoDETR">Code [250+ Stars ğŸŒŸ]</a><br /></p>
  </li>
  </ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2303.08134.pdf">Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis</a> <br />
<b>R Zhang</b>, L Wang, Y Wang, P Gao, H Li, J Shi# <br />
ğŸš€ The first 0-parameter and 0-training method for 3D analysis</p>
<b>CVPR 2023</b>, <a href="https://github.com/ZrrSkywalker/Point-NN">Code [350+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf">Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</a> <br />
<b>R Zhang*</b>, X Hu*, B Li, S Huang, H Deng, H Li, Y Qiao, P Gao# <br />
ğŸš€ Collaborate large models (GPT, CLIP, DINO, DALL-E) for image understanding</p>
<b>CVPR 2023</b>, <a href="https://github.com/ZrrSkywalker/CaFo">Code [300+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.pdf">Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders</a> <br />
<b>R Zhang</b>, L Wang, Y Qiao, P Gao, H Li# <br />
ğŸš€ Transfer pre-trained 2D knowledge into 3D space for point cloud learning</p>
<b>CVPR 2023</b>, <a href="https://github.com/ZrrSkywalker/I2P-MAE">Code [150+ Stars ğŸŒŸ]</a><br /></p>
</li>
</ul>
<ul>
  <li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25922">Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation</a> <br />
    Y Gan, X Ma, Y Lou, Y Bai, <b>R Zhang</b>, N Shi, L Luo# <br />
    ğŸš€ Visual prompting tuning for superior continual test-time adaptation</p>
    <b>AAAI 2023 Best Student Paper ğŸ’¥</b></p>
    </li>
  </ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ad1d7a4df30a9c0c46b387815a774a84-Paper-Conference.pdf">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</a> <br />
  <b>R Zhang</b>, Z Guo, P Gao, R Fang, B Zhao, D Wang, Y Qiao, H Li# <br />
  ğŸš€ Hierarchical 3D transformer for MAE-style self-supervised point cloud pre-training</p>
  <b>NeurIPS 2022</b>, <a href="https://github.com/ZrrSkywalker/I2P-MAE">Code [150+ Stars ğŸŒŸ]</a><br /></p>
  </li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2207.09519.pdf">Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification</a> <br />
  <b>R Zhang*</b>, W Zhang*, R Fang, P Gao, K Li, J Dai, Y Qiao, H Li# <br />
  ğŸš€ Propose a training-free cache model to adapt CLIP for downstream Visual recognition</p>
  <b>ECCV 2022</b>, <a href="https://github.com/gaopengcuhk/Tip-Adapter">Code [400+ Stars ğŸŒŸ]</a><br /></p>
  </li>
</ul>
<ul>
  <li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a> <br />
    <b>R Zhang*</b>, Z Guo*, W Zhang, K Li, X Miao, B Cui, Y Qiao, P Gao, H Li# <br />
    ğŸš€ The first work to extend CLIP for zero-shot 3D understanding</p>
    <b>CVPR 2022</b>, <a href="https://github.com/ZrrSkywalker/PointCLIP">Code [250+ Stars ğŸŒŸ]</a><br /></p>
    </li>
  </ul>
<h2>Selected Awards</h2>
<ul>
  <li><p>[2021-06] Outstanding Graduate, Peking University (Top 5%)</p>
  </li>
  <li><p>[2020-09] Academic Excellent Scholarship (Ranked 1st/73)</p>
  </li>
  <li><p>[2020-09] <b>Merit Student PaceSetter, Peking University (Ranked 1st/73)</b></p>
  </li>
  <li><p>[2019-09] Academic Excellent Scholarship (Ranked 4th/73)</p>
  </li>
  <li><p>[2019-09] Merit Student, Peking University (Ranked 4th/73)</p>
  </li>
  <li><p>[2016-07] <b>China Youth Technology Innovation Award (The Only 1 in Province)</b></p>
  </li>
  <li><p>[2016-10] 1st Prize in Provincial Chinese Physics Olympiad (Ranked 18th in Province)</p>
  </li>
  <li><p>[2015-10] 2nd Prize in The Chinese 15th Awarding Program for Future Scientist (Ranked 1st in Province)</p>
  </li>
  <li><p>[2013-03] 1st Prize in Provincial China Adolescent Robotics Competition (Ranked 1st in Province)</p>
  </li>
  </ul>
<h2>Hobbies</h2>
Soccer âš½ï¸, Moive ğŸ¬, Singing ğŸ¤, Piano ğŸ¹, Violin ğŸ», Games ğŸ®, Snorkeling ğŸ¤¿, <a href="https://www.hottoys.com.hk/">HotToys</a> ğŸ¦¸â€â™‚ï¸
<ul>
</ul>
</td>
</tr>
</table>
<a href="https://clustrmaps.com/site/1bvnp"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=PMNb1-V7zscdFbh7YtW8384Mvl0JzHFryZH1_dEVw5M&cl=ffffff" /></a>
</body>
</html>
