<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Renrui Zhang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-content">
<div id="toptitle">
<table class="imgtable"><tr><td>
<img src="bioo.jpg" alt="alt text" width="213px" height="160px" />&nbsp;</td>
<td align="left"><p><h1>Renrui Zhang</h1><br />Ph.D. candidate<br /> <br /> 
<a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory (MMLab)</a>, The Chinese University of Hong Kong<br />
Office: SHB 304, CUHK, Hong Kong S.A.R., China <br /> <br />
Email: 1700012927@pku.edu.cn, zhangrenrui@pjlab.org.cn <br /><br />
[<a href="https://scholar.google.com/citations?user=YlL3xN4AAAAJ&hl=zh-CN&authuser=1">Google Scholar</a>], [<a href="https://github.com/ZrrSkywalker">GitHub</a>] </p>
</td></tr></table>
<!-- <h2>Self-intro</h2> -->
  </ul>

  <ul>
📌 My research interets are multi-modal learning, large language/vision models, and 3D computer vision
</ul>
<ul>
✉️ Welcome to contact me for any discussion and cooperation!
</ul>

</ul>
<h2>Education</h2>
<ul>
<li><p>[2017-2021] 🎉 I received my B.E. degree from <a href="https://english.pku.edu.cn/">Peking University</a>, awarded Outstanding Graduate (Top 5%).</p>
</li>
<li><p>[2020-2021] I worked as a visiting student in University of Pennsylvania, supervised by <a href="https://scholar.google.com/citations?user=Sm14jYIAAAAJ&hl=zh-CN&oi=ao">Prof. Jianbo Shi</a>.</p>
</li>
<li><p>[2021-Now] 💪 I'm pursing my Ph.D. in MMLab, CUHK, supervised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a> and <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=zh-CN">Prof. Xiaogang Wang</a>.</p>
</li>
<li><p>[2021-Now] I'm working as a research intern at Shanghai AI Lab, supervised by <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=zh-CN">Dr. Peng Gao</a>.</p>
</li>
</ul>
<h2>News</h2>
<ul>
<li><p>[2023-09] One paper accepted by NeurIPS 2023</p>
</li>
<li><p>[2023-08] Two papers accepted by IJCV 2023</p>
</li>
<li><p>[2023-07] Five papers accepted by ICCV 2023</p>
</li>
<li><p>[2023-04] One paper accepted by IJCAI 2023</p>
</li>
<li><p>[2023-02] Six papers accepted by CVPR 2023</p>
</li>
<li><p>[2022-11] Two papers accepted by AAAI 2023</p>
</li>
<li><p>[2022-09] One paper accepted by NeurIPS 2022</p>
</li>
<li><p>[2022-07] Three papers accepted by ECCV 2022</p>
</li>
<li><p>[2022-03] One paper accepted by CVPR 2022</p>
</li>
<li><p>[2021-09] One paper accepted by NeurIPS 2021</p>
</li>
</ul>
<h2>Selected Preprints and Projects</h2>
<p>* Equal contribution, # Corresponding author</p>
<ul>
<li><p><a href="https://arxiv.org/pdf/2303.16199.pdf">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a> <br />
<b>R Zhang*</b>, J Han*, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao#, Y Qiao <br />
🚀 Fine-tune LLaMA for instruction-following capacity and multi-modal reasoning by PEFT methods</p>
<a href="https://github.com/OpenGVLab/LLaMA-Adapter">🔥 Code [4.7k+ Stars 🌟]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.03048.pdf">Personalize Segment Anything Model with One Shot</a> <br />
<b>R Zhang</b>, Z Jiang*, Z Guo*, S Yan, J Pan, H Dong, P Gao, H Li# <br />
🚀 Efficiently personalize SAM for segmenting specific objects and improving DreamBooth</p>
<a href="https://github.com/ZrrSkywalker/Personalize-SAM">🔥 Code [1.2k+ Stars 🌟]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://github.com/ZrrSkywalker/Point-Bind_Point-LLM/blob/main/Point-Bind%20%26%20Point-LLM.pdf">Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</a> <br />
Z Guo*, <b>R Zhang*#</b>, X Zhu, Y Tang, X Ma, J Han, K Chen, P Gao, X Li#, H Li, P Heng <br />
🧠 A 3D multi-modal model for general 3D learning, Point-Bind, and the first 3D large language model, Point-LLM</p>
<a href="https://github.com/ZrrSkywalker/Point-Bind_Point-LLM">🔥 Code [200+ Stars 🌟]</a><br /></p>
</li>
</ul>
<ul>
  <li><p><a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">LLaMA2-Accessory</a> <br />
  🧠 An open-source toolkit for the deployment of Large Language Models (LLMs) and multi-modal LLMs</p>
  <a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">🔥 Code [1.3k+ Stars 🌟]</a><br /></p>
  </li>
  </ul>
<h2>Selected Publications</h2>
<p>* Equal contribution, # Corresponding author</p>
<ul>
  <li><p><a href="https://arxiv.org/pdf/2203.13310.pdf">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</a> <br />
  <b>R Zhang</b>, H Qiu, T Wang, Z Guo, Z Cui, Y Qiao, P Gao, H Li# <br />
  🚀 The first DETR-based network for camera-based 3D object detection</p>
  <b>ICCV 2023</b>, <a href="https://github.com/ZrrSkywalker/MonoDETR">Code [200+ Stars 🌟]</a><br /></p>
  </li>
  </ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2303.08134.pdf">Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis</a> <br />
<b>R Zhang</b>, L Wang, Y Wang, P Gao, H Li, J Shi# <br />
🚀 The first 0-parameter and 0-training method for 3D analysis</p>
<b>CVPR 2023</b>, <a href="https://github.com/ZrrSkywalker/Point-NN">Code [200+ Stars 🌟]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf">Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</a> <br />
<b>R Zhang*</b>, X Hu*, B Li, S Huang, H Deng, H Li, Y Qiao, P Gao# <br />
🚀 Collaborate large models (GPT, CLIP, DINO, DALL-E) for image understanding</p>
<b>CVPR 2023</b>, <a href="https://github.com/ZrrSkywalker/CaFo">Code [200+ Stars 🌟]</a><br /></p>
</li>
</ul>
<ul>
<li><p><a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.pdf">Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders</a> <br />
<b>R Zhang</b>, L Wang, Y Qiao, P Gao, H Li# <br />
🚀 Transfer pre-trained 2D knowledge into 3D space for point cloud learning</p>
<b>CVPR 2023</b>, <a href="https://github.com/ZrrSkywalker/I2P-MAE">Code [100+ Stars 🌟]</a><br /></p>
</li>
</ul>
<ul>
  <li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25922">Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation</a> <br />
    Y Gan, X Ma, Y Lou, Y Bai, <b>R Zhang</b>, N Shi, L Luo# <br />
    🚀 Visual prompting tuning for superior continual test-time adaptation</p>
    <b>AAAI 2023 Best Student Paper 💥</b></p>
    </li>
  </ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ad1d7a4df30a9c0c46b387815a774a84-Paper-Conference.pdf">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</a> <br />
  <b>R Zhang</b>, Z Guo, P Gao, R Fang, B Zhao, D Wang, Y Qiao, H Li# <br />
  🚀 Hierarchical 3D transformer for MAE-style self-supervised point cloud pre-training</p>
  <b>NeurIPS 2022</b>, <a href="https://github.com/ZrrSkywalker/I2P-MAE">Code [100+ Stars 🌟]</a><br /></p>
  </li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2207.09519.pdf">Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification</a> <br />
  <b>R Zhang*</b>, W Zhang*, R Fang, P Gao, K Li, J Dai, Y Qiao, H Li# <br />
  🚀 Propose a training-free cache model to adapt CLIP for downstream Visual recognition</p>
  <b>ECCV 2022</b>, <a href="https://github.com/gaopengcuhk/Tip-Adapter">Code [300+ Stars 🌟]</a><br /></p>
  </li>
</ul>
<ul>
  <li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a> <br />
    <b>R Zhang*</b>, Z Guo*, W Zhang, K Li, X Miao, B Cui, Y Qiao, P Gao, H Li# <br />
    🚀 The first work to extend CLIP for zero-shot 3D understanding</p>
    <b>CVPR 2022</b>, <a href="https://github.com/ZrrSkywalker/PointCLIP">Code [200+ Stars 🌟]</a><br /></p>
    </li>
  </ul>
<h2>Selected Awards</h2>
<ul>
  <li><p>[2021-06] Outstanding Graduate, Peking University (Top 5%)</p>
  </li>
  <li><p>[2020-09] Academic Excellent Scholarship (Ranked 1st/73)</p>
  </li>
  <li><p>[2020-09] <b>Merit Student PaceSetter, Peking University (Ranked 1st/73)</b></p>
  </li>
  <li><p>[2019-09] Academic Excellent Scholarship (Ranked 4th/73)</p>
  </li>
  <li><p>[2019-09] Merit Student, Peking University (Ranked 4th/73)</p>
  </li>
  <li><p>[2016-07] <b>China Youth Technology Innovation Award (The Only 1 in Province)</b></p>
  </li>
  <li><p>[2016-10] 1st Prize in Provincial Chinese Physics Olympiad (Ranked 18th in Province)</p>
  </li>
  <li><p>[2015-10] 2nd Prize in The Chinese 15th Awarding Program for Future Scientist (Ranked 1st in Province)</p>
  </li>
  <li><p>[2013-03] 1st Prize in Provincial China Adolescent Robotics Competition (Ranked 1st in Province)</p>
  </li>
  </ul>
<h2>Hobbies</h2>
Soccer ⚽️, Moive 🎬, Singing 🎤, Piano 🎹, Violin 🎻, Games 🎮, Snorkeling 🤿, <a href="https://www.hottoys.com.hk/">HotToys</a> 🦸‍♂️
<ul>
</ul>
</td>
</tr>
</table>
<a href="https://clustrmaps.com/site/1bvnp"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=PMNb1-V7zscdFbh7YtW8384Mvl0JzHFryZH1_dEVw5M&cl=ffffff" /></a>
</body>
</html>
